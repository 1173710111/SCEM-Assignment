---
title: "TaskB1"
author: "zerofrom"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document:
    mathjax: default
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
```
## B.1     
### (1) Value of a
Considering chat $x$ follows the probability density function:

$$
p_{\lambda}(x)= \begin{cases}a e^{-\lambda(x-b)} & \text { if } x \geqslant b,  \tag{1}\\ 0 & \text { if } x<b,\end{cases}
$$

The probability density function $p_{\lambda}(x)$ must follows:

$$
\begin{equation}
\int_{-\infty}^{\infty} p_{\lambda}(x) d x=1 \tag{2}
\end{equation}
$$

Since $p_{\lambda}(x)=0$ when $x<b$, it follows that:

$$
\begin{equation}
\int_{b}^{\infty} p_{\lambda}(x) d x=1 \tag{3}
\end{equation}
$$

Replacing expression (1) into expression (3) yields:

$$
\begin{equation}
\int_{b}^{\infty} a e^{-\lambda(x-b)} d x=1 \tag{4}
\end{equation}
$$

Assuming that $u=x-b$, it can be obtained that:

$$
\begin{equation}
x=u+b, d x=d u \tag{5}
\end{equation}
$$

Assuming that $x=b$, then $u=0$, therefore the upper and lower limits of integration become $0 \rightarrow \infty$, substituting expression $(s)$ :

$$
\begin{equation}
\int_{0}^{\infty} a e^{-\lambda u} d u=1 \tag{6}
\end{equation}
$$

Using the exponential integration formula:

$$
\begin{equation}
\int_{0}^{\infty} e^{-a u} d u=\frac{1}{a} \tag{7}
\end{equation}
$$

Thus:

$$
\begin{equation}
a \cdot \frac{1}{\lambda}=1 \tag{8}
\end{equation}
$$

The solution is:

$$
\begin{equation}
a=\frac{1}{\lambda} \tag{9}
\end{equation}
$$

### In conclusion, the value of a is $\frac{1}{\lambda}$.      


#### (2) i) Population Mean $E[X]$ :

From the definition of the mean, there is:

$$
\begin{equation}
E[x]=\int_{-\infty}^{\infty} x \cdot p_{\lambda}(x) d x \tag{1}
\end{equation}
$$

Replacing $p_{\lambda}(x)$ and $a=\lambda$ :

$$
\begin{equation}
E[x]=\int_{b}^{\infty} x \cdot \lambda e^{-\lambda(x-b)} d x \tag{2}
\end{equation}
$$

Assuming $u=x-b$, then $x=u+b$, and $d x=d u$ :

$$
\begin{align*}
E[X] &= \int_{0}^{\infty}(u + b) \cdot \lambda e^{-\lambda u} \, du \\
     &= \lambda \cdot \int_{0}^{\infty} u e^{-\lambda u} \, du + \lambda b \int_{0}^{\infty} e^{-\lambda u} \, du \tag{3}
\end{align*}
$$

Integral formula for exponential functions:

$$
\begin{equation}
\int_{0}^{\infty} e^{-a x} d x=\frac{1}{a}, \int_{0}^{\infty} x e^{-a x} d x=\frac{1}{a^{2}} \tag{4}
\end{equation}
$$

Replacing expression (t) into expression (3), there is:

$$
\begin{aligned}
E[x] & =\lambda \cdot \frac{1}{\lambda^{2}}+\lambda b \cdot \frac{1}{\lambda} \\
& =\frac{1}{\lambda}+b
\end{aligned}
$$

### In conclusion, the population mean $E[x]=\frac{1}{\lambda}+b$.    

#### ii) Standard Deviation $\sigma x$ :

The standard deviation formula:

$$
\begin{equation}
\sigma x=\sqrt{\operatorname{Var}(x)}=\sqrt{E\left[x^{2}\right]-(E[x])^{2}} \tag{1}
\end{equation}
$$

From the derivation of $E[x]$, it follows that:

$$
E[x]=\frac{1}{\lambda}+b \tag{2}
$$

The formula for $E\left[x^{2}\right]$ is:

$$
\begin{equation}
E\left[x^{2}\right]=\int_{b}^{\infty} x^{2} \cdot p_{\lambda}(x) d x \tag{3}
\end{equation}
$$

Substituting $P_{\lambda}(x)$ :

$$
\begin{equation}
E\left[x^{2}\right]=\int_{b}^{\infty} x^{2} \cdot \lambda e^{-\lambda(x-b)} d x \tag{4}
\end{equation}
$$

Assuming $u=x-b$. then $x=u+b$, and $d x=d u$ :

$$
\begin{align*}
E\left[x^{2}\right]= & \int_{0}^{\infty}(u+b)^{2} \cdot \lambda e^{-\lambda u} d u \\
= & \lambda \int_{0}^{\infty}\left(u^{2}+2 b u+b^{2}\right) \cdot e^{-\lambda u} d u \\
= & \lambda \int_{0}^{\infty} u^{2} e^{-\lambda u} d u+2 \lambda b \int_{0}^{\infty} u e^{-\lambda u} d u  +\lambda b^{2} \int_{0}^{\infty} e^{-\lambda u} d u \quad  \tag{5}
\end{align*}
$$

Integrals formula for exponential functions:

$$
\begin{align*}
& \int_{0}^{\infty} e^{-a x} d x=\frac{1}{a} \\
& \int_{0}^{\infty} x e^{-a x} d x=\frac{1}{a^{2}} \\
& \int_{0}^{\infty} x^{2} e^{-a x} d x=\frac{2}{a^{3}} \tag{6}
\end{align*}
$$

Replacing formulas (6) into expression (5), there is:

$$
\begin{align*}
E\left[x^{2}\right] & =\lambda \cdot \frac{2}{\lambda^{3}}+2 \lambda b \cdot \frac{1}{\lambda^{2}}+\lambda b^{2} \cdot \frac{1}{\lambda} \\
& =\frac{2}{\lambda^{2}}+\frac{2 b}{\lambda}+b^{2} \tag{7}
\end{align*}
$$

Replacing expression (2) and (7) into expression (1):

$$
\begin{align*}
\sigma_{x} & =\sqrt{\left(\frac{2}{\lambda^{2}}+\frac{2 b}{\lambda}+b^{2}\right)-\left(\frac{1}{\lambda}+b\right)^{2}} \\
& =\sqrt{\left(\frac{2}{\lambda^{2}}+\frac{2 b}{\lambda}+b^{2}\right)-\left(\frac{1}{\lambda^{2}}+\frac{2 b}{\lambda}+b^{2}\right)} \\
& =\frac{1}{\lambda}
\end{align*}
$$

### In conclusion, the standard deviation of $X$ is $\sigma_{x}=\frac{1}{\lambda}$.    

#### (3) i) Cumulative distribution function (CDF): Considering the definition of CDF, it follows that:

$$
\begin{equation}
F_{x}(x)=P(x \leqslant x)=\int_{-\infty}^{\infty} p_{x}(t) d t . \tag{1}
\end{equation}
$$

If $x<b$, then $p_{x}(t)=0$, thus there is:

$$
\begin{equation}
F_{X}(x)=\int_{-\infty}^{x} 0 d t=0 \tag{2}
\end{equation}
$$

If $x \geqslant b$, then $p_{x}(t)=\lambda e^{-\lambda(t-b)}$, thus there is:

$$
\begin{equation}
F_{X}(x)=\int_{b}^{x} \lambda e^{-\lambda(t-b)} d t \tag{3}
\end{equation}
$$

Assuming that $a=t-b$, then $t=u+b$. and $d t=d u$, the upper and Cower limit of integration becomes $0 \rightarrow x-b$.

$$
\begin{aligned}
F_{X}(x) & =\int_{0}^{x-b} \lambda e^{-\lambda u} d u \\
& =\lambda \cdot \int_{0}^{x-b} e^{-\lambda u} d u \\
& =\lambda\left[-\frac{1}{\lambda} e^{-\lambda u}\right]_{0}^{x-b} \\
& =\lambda \cdot\left(-\frac{1}{\lambda} e^{-\lambda(x-b)}+\frac{1}{\lambda} e^{0}\right) \\
& =\lambda \cdot\left(-\frac{1}{\lambda} e^{-\lambda(x-b)}+\frac{1}{\lambda}\right) \\
& =-e^{-\lambda(x-b)}+1 
\end{aligned}
$$



### In conclusion, the cumulative distribution function for the random variable $x$ is $F_{x}(x)= \begin{cases}0, & \text { if } x<b \\ 1-e^{-x(x-b)}, & \text { if } x \geqslant b\end{cases}$    


#### ii) Quartile Function (QF)

Considering the CDF of random variable $X$ :

$$
F_{X}(x)=\left\{\begin{array}{l}
0, \text { if } x<b  \tag{1}\\
1-e^{-\lambda(x-b),} \text { if } x \geqslant b
\end{array}\right.
$$

Quartile Function $Q(p)$ satisfies:

$$
\begin{equation}
F_{X}(Q(p))=p \text {, if } 0<p<1 \tag{2}
\end{equation}
$$

Replacing (1) into cl), it follows that:

$$
\begin{equation}
1-e^{-\lambda(x-b)}=p \tag{3}
\end{equation}
$$

Thus there is:

$$
\begin{equation}
e^{-\lambda(x-b)}=1-p \tag{4}
\end{equation}
$$

Taking the natural logarithm of both sides:

$$
\begin{equation}
-\lambda(x-b)=\ln (1-p) \tag{5}
\end{equation}
$$

Thus 
$$
\begin{equation}
x=b-\frac{\ln (1-p)}{\lambda} \tag{6}
\end{equation}
$$
### In conclusion, the Quartile Function of random variable $X$ is $Q(p)=b-\frac{\ln (1-p)}{\lambda}, 0<p<1$    

#### (4) Maxmum Likehood estimat \MLE:

The definition of likehood function is:

$$
\begin{align*}
L(\lambda) & =\prod_{i=1}^{n} p_{\lambda}\left(x_{i}\right) \\
& =\prod_{i=1}^{n} \lambda e^{-\lambda\left(x_{i}-b\right)} \\
& =\lambda^{n} \prod_{i=1}^{n} e^{-\lambda\left(x_{i}-b\right)} \\
& =\lambda^{n} e^{-\lambda \sum_{i=1}^{n}\left(x_{i}-b\right)} \tag{1}
\end{align*}
$$

Taking the natural logarithm of both sides:

$$
\begin{equation}
\ln L(\lambda)=n \ln \lambda-\lambda \sum_{i=1}^{n}\left(x_{i}-b\right) \tag{2}
\end{equation}
$$

Derivativing with respect to $\lambda$ :

$$
\begin{equation}
\frac{d \ln L(\lambda)}{d \lambda}=\frac{n}{\lambda}-\sum_{i=1}^{n}\left(x_{i}-b\right) \tag{3}
\end{equation}
$$

Assuming that the derivative is zero, it follows that:

$$
\begin{equation}
\frac{n}{\lambda}-\sum_{i=1}^{n}\left(x_{i}-b\right)=0 \tag{4}
\end{equation}
$$

Calculating the value of $\lambda$ is:

$$
\begin{equation}
\lambda=\frac{n}{\sum_{i=1}^{n}\left(x_{i-b}\right)} \tag{5}
\end{equation}
$$

#### In conclusion, the maxmum likehood estimate for $x$ is $\lambda_{\text {MLE }}=\frac{n}{\sum_{i=1}^{n}\left(x_{i}-b\right)}$

### (5) Given the sample, compute and display the maximum likelihood estimate λMLE of the parameter λ. 
```{r}
#Creating a lambda value calculation function
lambda_function <- function(data, b){
  n <- length(data)
  # Calculating the denominator: summing over x - b
  sum_i <- sum(data-b)
  # Calculating lembda
  lambda <- n/sum_i
  return(lambda)
}

# Load Data
supermarket_data <- read.csv("data\\supermarket_data_2024.csv")
# Set the number of sample
n <- nrow(supermarket_data)
# Set the constant b
b <-300

lambda_mle <- lambda_function(supermarket_data$TimeLength , b)
print(paste("The maximum likelihood estimate lambda_MLE is: ",lambda_mle))
```
### (6) Compute the Bootstrap confidence interval
```{r}
# set the random seed
set.seed(2024)
# set the number of resamples
number_resamples <- 10000

lambda_resamples <- numeric(number_resamples)
# Sampling with replacement
for (i in 1:number_resamples){
  sample_data <- sample(supermarket_data$TimeLength , replace = TRUE)
  lambda_resamples[i] <- lambda_function(sample_data, b)
}

#Calculate the 95% confidence interval
  ci_lower <- quantile(lambda_resamples, 0.025)
  ci_upper <- quantile(lambda_resamples, 0.975)
  print(paste("The Bootstrap confidence interval is: [",ci_lower,',',ci_upper,']\n'))
  
```
### (7)) Conduct a simulation study to explore the behaviour of the maximum likelihood estimator λMLE for λ on simulated data X1, · · · , Xn
```{r}
# Simulation parameters
set.seed(2024)
lambda_simulation <- 2
b_simulation <- 0.01
sampleSize_simulation <- seq(100, 5000, by = 10)
repetition_simulation <- 100

# Simulation results
simulation_results <- data.frame(SampleSize = sampleSize_simulation, MSE = numeric(length(sampleSize_simulation)))


# Start simulation
index <- 1
for (n in sampleSize_simulation){
  lambda_estimates<-numeric(repetition_simulation)
  #For each sample size, consider 100 trials.
  for(i in 1:repetition_simulation){
    sample_data <- rexp(n, rate = lambda_simulation) + b_simulation
    lambda_estimates[i] <- lambda_function(sample_data,b_simulation)
  }
  #Calculate mean and Mean Squared Error(MSE)
  mse <- mean((lambda_estimates-lambda_simulation)^2)
  simulation_results[index, "MSE"] <- mse
  index <- index+1
}

#Check the result
head(simulation_results,5)

#Plotting results of the simulation
ggplot(simulation_results,
       aes(x=SampleSize, y=MSE))+
  geom_line() +
  labs(x="Sample Size", y="Mean Squared Error")
```



## B2
### (1) The formula for the probability mass function
The range of possible values of the discrete ranom variable $x$ is:

$$
x \in\{-2,0,2\}
$$

Event 1: Draw 2 blue balls $(X=-2)$

$$
\begin{aligned}
& C(b, 2)=\frac{b(b-1)}{2}, C(a+b, 2)=\frac{(a+b)(a+b-1)}{2} \\
& \therefore P(x=-2)=\frac{C(b, 2)}{C(a+b, 2)}=\frac{b(b-1)}{(a+b)(a+b-1)}
\end{aligned}
$$

Event 2: Draw 1 red ball and 1 blue ball $(x=0)$

$$
\begin{aligned}
& C(a, 1) \cdot C(b, 1)+C(b, 1) \cdot C(a, 1)=2 \cdot a \cdot b \\
\therefore  & P(X=0)=\frac{C(a, 1) C(b, 1)+C(b, 1) C(a, 1)}{C(a+b, 2)}\\
&=\frac{2 a b}{(a+b)(a+b-1)}
\end{aligned}
$$

Event 3: Draw 2 red balls $(x=2)$

$$
\begin{aligned}
C(a, 2) & =\frac{a(a-1)}{2} \\
\therefore P(x=2) & =\frac{C(a, 2)}{C(a+b, 2)}=\frac{a(a-1)}{(a+b)(a+b-1)}
\end{aligned}
$$

### In conclustion, the formula for the probability mass function pros:

$$
P(x)= \begin{cases}\frac{b(b-1)}{(a+b)(a+b-1)} & , x=-2 \\ \frac{2a b}{(a+b)(a+b-1)} & , x=0 \\ \frac{a(a-1)}{(a+b)(a+b-1)} & , x=2\end{cases}
$$

### (2) The expression of the expectation
According to the expectation formula for random variables:

$$
\begin{equation}
E(X)=\sum_{x \in\{-2,0,2\}} x \cdot P(X=x) \tag{1}
\end{equation}
$$

Replacing $P(x)$ into (1) , there is:

$$
\begin{align*}
E(x) & =-2 \cdot \frac{b(b-1)}{(a+b)(a+b-1)}+2 \cdot \frac{a(a-1)}{(a+b)(a+b-1)} \\
& =\frac{2 a(a-1)-2 b(b-1)}{(a+b)(a+b-1)} \\
& =\frac{2\left(a^{2}-b^{2}-a+b\right)}{(a+b)(a+b-1)} \tag{2}
\end{align*}
$$

### In conclusion, the expectation $E(X)$ of $X$ is:

$$
E(X)=\frac{2\left(a^{2}-b^{2}-a+b\right)}{(a+b)(a+b-1)}
$$

### (3)The expression of the variance Var(X)
According to the formula of $E\left(X^{2}\right)$ :

$$
\begin{equation}
E\left(X^{2}\right)=\sum_{x \in\{-2,0,2\}} x^{2} \cdot P(X=x) \tag{1}
\end{equation}
$$

Replacing $P(x)$ in to $(1)$, there is:

$$
\begin{align*}
E\left(x^{2}\right) & =4 \cdot \frac{b(b-1)}{(a+b)(a+b-1)}+4 \cdot \frac{a(a-1)}{(a+b)(a+b-1)} \\
& =\frac{4\left(a^{2}+b^{2}-a-b\right)}{(a+b)(a+b-1)} \tag{2}
\end{align*}
$$

According to the formula of $\operatorname{Var}(X)$ :

$$
\begin{equation}
\operatorname{Var}(x)=E\left(x^{2}\right)-[E(x)]^{2} \tag{3}
\end{equation}
$$

Replacing $E\left(x^{2}\right)$ and $E(x)$ into (3), it follows that:

$$
\begin{aligned}
\operatorname{Var}(x) & =\frac{4\left(a^{2}+b^{2}-a-b\right)}{(a+b)(a+b-1)}-\left[\frac{2\left(a^{2}-b^{2}-a+b\right)}{(a+b)(a+b-1)}\right]^{2} \\
& =\frac{4\left(a^{2}+b^{2}-a-b\right)(a+b)(a+b-1)-4\left(a^{2}-b^{2}-a+b\right)^{2}}{(a+b)^{2}(a+b-1)^{2}}
\end{aligned}
$$
### In conclusion, the expression of the variance Var(X) is:    
$$
\begin{aligned}
\operatorname{Var}(x) & =\frac{4\left(a^{2}+b^{2}-a-b\right)(a+b)(a+b-1)-4\left(a^{2}-b^{2}-a+b\right)^{2}}{(a+b)^{2}(a+b-1)^{2}}
\end{aligned}
$$
### (4)Write a function called compute_expectation_X that takes a and b as inputs and outputs the expectation E(X). Write a function called compute_variance_X that takes a and b as input and outputs the variance Var(X).
```{r}
# The function of E(X)
compute_expectation_X <- function(a,b){
  f1 <- 2*(a^2-b^2-a+b)
  f2 <- (a+b)*(a+b-1)
  expectation = f1/f2
  return(expectation)
}

# The function of Var(x)
compute_variance_X <- function(a,b){
  #f1 <- (a + b) * (a + b - 1)                
  #f2 <- 4 * (a * (a - 1) + b * (b - 1)) / f1  
  #f3 <- 4 * (a^2 - b^2 - a + b)^2 / f1^2  
  #variance <- f2-f3           
  f1 <- (a+b)*(a+b-1)
  f2 <- (a^2+b^2-a-b)*f1
  f3 <- (a^2-b^2-a+b)^2
  variance <- (4*f2-4*f3)/f1^2
  return(variance)
}
```

### (5) The expression of the expectation of the random variable X
According to the linear nature of the mean of a random variable:

$$
\begin{equation}
E(\bar{X})=E\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right)=\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right) \tag{1}
\end{equation}
$$

$\because X_{i}$ are i.i.d., thus $E\left(X_{i}\right)=E(x)$, thus:

$$
\begin{equation}
E(\bar{X})=\frac{1}{n} \cdot n \cdot E(X)=E(X) \tag{2} 
\end{equation}
$$    
 
### In conclusion, the expression of the expectation of the random variable $\bar{x}$ is: $E(\bar{X})=\frac{2\left(a^{2}-b^{2}-a+b\right)}{(a+b)(a+b-1)}$

### (6) The expression of the variance of the random variable X
Considering the effect of a linear transformation of a random variable on the variance :

$$
\begin{equation}
\operatorname{Var}(a X+b)=a^{2} \operatorname{Var}(X)  \tag{1}\\
\end{equation}
$$
thus:
$$
\begin{align*}
&\operatorname{Var}(\bar{X})=\operatorname{Var}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right) \\
&=\frac{1}{n^{2}} \operatorname{Var}\left(\sum_{i=1}^{n} X_{i}\right) \\
&=\frac{1}{n^{2}} \cdot n \operatorname{Var}(X) \\
&=\frac{1}{n} \operatorname{Var}(X) \tag{2}
\end{align*}
$$

### In conclusion, the expression of the variance of the random variable $\bar{X}$ is    

$$
\operatorname{Var}(\bar{X})=\frac{4\left(a^{2}+b^{2}-a-b\right)(a+b)(a+b-1)-4\left(a^{2}-b^{2}-a+b\right)^{2}}{n(a+b)^{2}(a+b-1)^{2}}
$$
### (7) Create a function called sample_Xs
```{r}
sample_Xs <-function(a,b,n){
  #define the probability density function
  f1 <-(a+b)*(a+b-1)
  p_x_minus2 <- (b*(b-1))/f1
  p_x_0 <- 2*(a*b)/f1
  p_x_2 <- (a*(a-1))/f1
  x_values <- c(-2,0,2)
  p_values <- c(p_x_minus2, p_x_0, p_x_2)
  
  #random sample
  samples <- sample(x_values, size = n, replace = TRUE, prob=p_values)
  return(samples)
}
```

### (8) Calculate E(X),Var(X),E(\bar{X}),Var(\bar{X})
```{r}
# set the value of parameters
a <- 3
b <- 5
n <- 100000
# E(X) and Var(X)
e_x <- compute_expectation_X(a,b)
var_x <- compute_variance_X(a,b)
# generate a sample
set.seed(2024)
samples <- sample_Xs(a,b,n)
#head(samples,10)

# E(\bar{X}) and Var(\bar{X})
e_bar_x <- mean(samples)
var_bar_x <-var(samples)

# 输出结果
print(paste("Expectation E(X):", e_x, "\n"))
print(paste("Expectation of samples E(bar{X}):", e_bar_x, "\n"))
print(paste("Variance Var(X):", var_x, "\n"))
print(paste("Variance of samples Var(bar{X}):", var_bar_x, "\n"))
```

### (9) Compute the corresponding sample mean X based on X1, · · · , Xn
```{r}
# Set the parameter
a <- 3
b <- 5
n <- 100
num_trials <- 50000
# Initialize the array of experimental sample means
mean_trails <- numeric(num_trials)
# Start simulation
set.seed(2024)
for (i in 1:num_trials){
  samples <- sample_Xs(a,b,n)
  mean_trails[i] <- mean(samples)
}
```

### (10) Create a scatter plot of the points
```{r}
# Calculate mu and sigma
mu <- compute_expectation_X(a,b)
sigma <- sqrt(compute_variance_X(a,b)/n)
# Normally distributed densities
density_x <- seq(mu - 3 * sigma, mu + 3 * sigma, by = 0.1*sigma)
density_points <- data.frame(
  x = density_x,
  y = dnorm(density_x, mean = mu, sd = sigma)
)
# Density estimation
density_trials <- data.frame(
  x = density(mean_trails)$x,
  y = density(mean_trails)$y
) 

# Create a scatter plot of the points
ggplot()+
  geom_point(data=density_points,
            aes(x=x, y=y, color="Normal Density Points"),
            size=0.2)+
  geom_line(data=density_trials,
            aes(x=x, y= y, color="Sample Mean Density Curve"),
            size=0.1)+
  labs(
    x="Mean bar{X}",
    y="Density",
  )+
  scale_color_manual(
     values = c(
       "Normal Density Points" = "blue",
       "Sample Mean Density Curve" = "red"
     )
  ) +
  theme_minimal()
```

### (11) Describe the relationship between the density of X and the function fµ,σ displayed in your plot. Try to explain the reason.

(i) The blue points in the figure depicts the density distribution of the mean of the discrete random variable X over the interval [µ - 3σ, µ + 3σ], satisfying the normal distribution. 
(ii) The red curve in the figure is the kernel density estimate obtained by representing the kernel density of the sample mean X within simulation study with 50000 trials. 
(iii) It can be seen that the two are very close to each other, proving that the sampling distribution of the sample means tends to the standard normal distribution when the sample size is large enough.